# Modul 4: AI Compute Hardware - Opsummering

**Kursus:** AI Systems & Infrastructure  
**Form√•l:** Forst√• computer arkitektur, begr√¶nsninger ved generisk hardware, og specialiseret hardware til AI computing

---

## üìö TL;DR

Moderne "AI computere" er ikke fundamentalt forskellige: de bruger stadig den 80-√•rige Von Neumann arkitektur. Mens CPUs excellerer ved sekventiel processing, har AI workloads brug for massive parallel computation og h√∏j memory bandwidth. Denne mismatch ledte til specialiseret hardware.

---

## üéØ Hovedbegreber

### AI Computers - Hype eller Realitet?

**Vigtig Pointe:**
- De fleste "AI computere" er stadig baseret p√• Von Neumann arkitektur fra 1945
- 80 √•r senere bruger de fleste computere p√• Jorden stadig denne arkitektur
- Hardware capabilities er vokset drastisk, men grundl√¶ggende arkitektur forbliver den samme

**Hvorfor det er vigtigt:**
- Capabilities af computer hardware er vokset hurtigt siden arkitekturen blev introduceret
- Dette er en af de vigtigste motivationer og fundamenter for udviklingen af AI modeller og systemer
- Vi skal starte fra basics og se p√• arkitekturen der startede det hele

---

## üèóÔ∏è Computer Architecture

### Von Neumann Architecture (1945)

**Hvad er det?**
- Dokumenteret af John von Neumann i 1945
- Den mest indflydelsesrige computer arkitektur design i historien
- Etablerede fundamentet der stadig styrer de fleste computere i dag
- Fra smartphones til supercomputers

**Analog: Restaurant Kitchen**
- En travl restaurant k√∏kken med ordrer og opskrifter (instructions) der kommer ind
- Ingredienser (data) klar til at blive kogt
- Kokke (CPU) f√∏lger ordrer og opskrifter og forbereder retter
- Pantry og counter (memory unit) til at lagre ingredienser og opskrifter
- Tjenere (input/output devices) bringer ordrer ind og leverer retter
- Korridorer (bus) forbinder alt personale og rum

### Komponenter i Von Neumann Architecture

#### 1. Instruction & Data

**Instructions (Opskrifter):**
- Fort√¶ller computeren pr√¶cist hvilke operationer der skal udf√∏res
- Ligesom opskrifter i en restaurant
- Step-by-step guide p√• hvordan man h√•ndterer ingredienser og kogeredskaber

**Eksempel - Recipe:**
```
1. Cut onion into pieces
2. Heat up pan to medium heat
3. Add 2 tablespoons oil
4. Saut√© onions until golden
```

**Eksempel - Computer Instructions:**
```
1. LOAD dkk_price
2. MULTIPLY dkk_price by conversion_factor
3. STORE result in usd_price
4. DISPLAY usd_price
```

**Data (Ingredienser):**
- Repr√¶senterer informationen der skal processeres
- Ligesom ingredienser i en restaurant

**Eksempel:**
```
- 2 large onions
- Olive oil
- dkk_price: 599
- conversion_factor: 0.1570
- usd_price: to be calculated
```

#### 2. Central Processing Unit (CPU)

**Hvad er det?**
- Hjernen af computeren
- Ligesom gruppen af kokke i restauranten
- Kan v√¶re sammensat af forskellige typer sub-units, is√¶r i moderne CPUs

**To essentielle typer:**

1. **Control Unit (CU) - Executive Chef:**
   - L√¶ser ordrer og opskrifter
   - Forst√•r hvad der skal g√∏res for at opfylde ordrer
   - Koordinerer alt personale og udstyr til at udf√∏re hvert step
   - **Specifikke opgaver:**
     - Henter n√¶ste instruction fra memory
     - Fortolker instruction's operation code og operands
     - Koordinerer execution ved at sende signals til andre komponenter

2. **Arithmetic Logic Unit (ALU) - Kokke der laver maden:**
   - G√∏r det faktiske arbejde (cooking)
   - Processerer ingredienser efter kommandoer fra CU
   - **Kan h√•ndtere:**
     - Arithmetic (addition, subtraction, multiplication, division)
     - Logical (AND, OR, NOT, XOR)
     - Comparison (equals to, greater than, less than)
     - Bit manipulation (shifts, rotations, etc.)

#### 3. Memory

**Hvad er det?**
- Hvor b√•de instructions og data lagres
- Ligesom en omfattende pantry hvor b√•de ingredienser og opskriftb√∏ger lagres
- Har et address system, lignende pantry der har et unified shelving system

**Karakteristika:**
- **Unified address space:** B√•de instructions og data bruger samme addressing scheme
- **Random access:** Enhver memory location kan tilg√•s direkte i konstant tid
- **Volatile storage:** Indhold g√•r tabt n√•r str√∏mmen fjernes

#### 4. Input/Output (I/O)

**Hvad er det?**
- H√•ndterer kommunikation mellem computeren og den eksterne verden
- Ligesom tjenere i restauranten der bringer ordrer ind og leverer f√¶rdige retter

**Abstract Standpoint:**
- **I/O controllers:** Device management og protocol handling
- **I/O methods:** Forskellige typer interaktioner mellem I/O devices og computeren

**Physical Standpoint:**
- **Input devices:** Keyboard, mouse, trackpad, microphone, camera
- **Output devices:** Monitor, speaker, printer

#### 5. Bus

**Hvad er det?**
- Giver kommunikationsveje p√• tv√¶rs af alle komponenter i en computer
- Ligesom korridorer i k√∏kkenet for personale at bev√¶ge sig rundt, kommunikere, tilg√• forskellige komponenter, og b√¶re kogeredskaber, ingredienser og retter

**Tre sub-systemer:**

1. **Address Bus:**
   - Specificerer memory eller I/O device location der skal tilg√•s

2. **Data Bus:**
   - B√¶rer faktisk data transfereret mellem komponenter

3. **Control Bus:**
   - B√¶rer control signals og koordinerer forskellige komponenter

**Factorio Analogi:**
- For scalable production har du typisk ogs√• et bus system der forbinder storage boxes, I/O endpoints, og maskiner der producerer eller forbruger ting
- S√•dant system g√∏r det let at tilf√∏je et nyt sub-system til eksisterende

### Von Neumann Architecture i Praksis

**Eksempel: Raspberry Pi 5**

**Komponenter:**
- **CPU:** BCM2712 processor (center-left p√• boardet)
  - Har multiple cores (flere kokke arbejder sammen)
- **Memory (RAM):** Positioneret meget t√¶t p√• CPU
  - For at s√¶nke access latency
  - Ligesom at s√¶tte counters t√¶ttere betyder hurtigere adgang til ting kokke har brug for
- **I/O Interfaces:**
  - PCI Express interface for high-speed peripherals
  - Ethernet og USB connectors
  - MIPI DSI/CSI connectors til kameraer
  - Raspberry Pi RP1 I/O controller h√•ndterer forbindelser
- **Bus System:**
  - Traces overalt p√• boardet
  - Fysisk implementering af bus systemet
  - Essentielt kobbertr√•de der forbinder alle komponenter sammen

---

## ‚ö†Ô∏è Limitations of Generic Hardware

### Problem: CPU vs AI Workloads

**To hovedproblemer:**

#### 1. Sequential Processing vs. Parallel Demands

**CPUs:**
- Excellerer ved **sequential processing**
- Kan udf√∏re komplekse instructions en efter en
- **Analog:** Universitetsprofessor der kan l√∏se komplekse matematikproblemer
- Kan l√∏se ethvert kendt problem, men kun √©t problem ad gangen
- Moderne CPUs har typisk **8 cores** (consumer tier) eller **64 cores** (professional server tier)

**AI Models:**
- Tungt afh√¶ngige af **matrix-related computation**
- Matrix manipulation udg√∏r **45-60% af runtime** i Transformer modeller (store sprogmodeller)
- Manipulationer involverer typisk relativt simple instructions (add, multiply)
- Men hver manipulation inkluderer **tusindvis af uafh√¶ngige beregninger** der kunne ske i parallel
- **Analog:** Giv en professor tusind simple ligninger at l√∏se
  - Hver ligning er meget simpel for professoren
  - Men vil stadig tage meget tid at l√∏se dem alle
  - En gruppe (hundreder) af folkeskoleelever, selvom hver er inkompetent til at l√∏se komplekse ligninger, vil sandsynligvis v√¶re hurtigere til at h√•ndtere disse tusind ligninger

#### 2. Memory Bus Bottleneck

**CPU Memory Bus:**
- Designet til at v√¶re **low latency**
- CPUs er typisk ansvarlige for at udf√∏re komplekse instructions der involverer fetching og storing data spredt i forskellige lokationer i memory
- **Latency er vigtigere** for CPUs

**AI Models:**
- Tungt afh√¶ngige af **large-scale parallel instructions** p√• matricer
- Matricer er typisk lagret i en relativt lokal blok i memory
- **Low latency memory bus' fordel bliver ulempe her**
- Low latency memory bus kommer typisk med ulempen af **low bandwidth**
- **Bandwidth er mere kritisk** for de fleste AI modeller
- Evnen til at flytte en stor chunk af data hurtigt er en mere kritisk metrik

**Konklusion:**
- Fundamentalt mismatch mellem CPU arkitektur og AI workload
- Kalder p√• specialiseret hardware til at speede op AI computing
- Vi har brug for hardware der excellerer i parallel processing og har high-bandwidth memory

---

## üöÄ Specialized Hardware

### Graphics Processing Unit (GPU)

**Oprindelse:**
- Oprindeligt designet til at processere computer graphics
- Oprindeligt designet i 1980'erne til at accelerere 3D graphics rendering for videospil
- Rendering af 3D videospil involverer beregning af lighting, shading, og texture mapping
- Viser millioner af pixels med h√∏jt optimerede algoritmer der bryder s√•danne beregninger i sm√• units
- Units er sammensat af simple instructions og kan g√∏res i parallel

**Design:**
- Excellerer ved **parallel processing**
- **Moderne CPU:** Mindre end 100 kraftfulde cores
- **Moderne GPU:** Tusindvis af svage cores
- Hver core kan kun h√•ndtere simple instructions‚Äîligesom en folkeskoleelev
- Men alle cores kombineret kan f√¶rdigg√∏re en paralleliseret opgave meget hurtigere end en CPU

**Memory:**
- Designet omkring **high-bandwidth**
- Store chunks af data kan tilg√•s hurtigt
- **DDR memory (CPUs):** ~50-100 GB/s bandwidth
- **GDDR memory (GPUs):** Op til **1.5 TB/s bandwidth**
- **HBM memory (AI workloads):** Op til **2 TB/s bandwidth**

**AI Computing:**
- Behovet for parallel processing og high-bandwidth af computer graphics aligner godt med AI computing
- GPU er blevet den dominerende type specialiseret hardware for AI workloads i de seneste √•r
- Desv√¶rre betyder dette at store GPU brands ikke l√¶ngere giver en skid for gamere og generelle forbrugere

### Tensor Processing Unit (TPU)

**Hvad er det?**
- Google's TPU (Tensor Processing Unit)
- Hardware specifikt designet til AI computing
- Introduceret som AI industrien vokser hurtigt

**Arkitektur:**
- Tusindvis af simple processor cores aligned i et grid
- Indkommende data og instructions flyder gennem gridet som b√∏lger
- Hver processor core laver en lille beregning og passerer resultatet til sine naboer

**Fordele:**
- H√∏jt specialiseret i AI computing
- Kan v√¶re mere effektiv for AI workloads sammenlignet med GPU
- GPU skal stadig h√•ndtere graphics og andre generelle computing tasks

**Ulemper:**
- Upraktisk for alle andre opgaver
- Nu om dage prim√¶rt set i data centers, is√¶r dem bygget af Google selv

### Neural Processing Unit (NPU)

**Hvad er det?**
- Specialiseret AI computing hardware i personlige computing devices (PCs, smartphones)
- Fokuserer p√• **power efficiency**
- M√•l: Levere AI computing acceleration mens den forbruger minimal power og fysisk plads

**Karakteristika:**
- **Miniaturisering:**
  - Fokuserer p√• at k√∏re pre-trained modeller i stedet for at tr√¶ne nye
  - Bruger typisk low-precision arithmetic (8-bit eller endda 4-bit) sammenlignet med fuld 32-bit
- **Forskellige designs:**
  - **Apple:** Neural Engine (integreret i smartphone chips fra iPhone 8)
  - **Qualcomm:** AI Engine (arbejder samarbejdende med GPUs i deres chips)

**Moderne Eksempler:**
- Apple's M4 desktop chip
- AMD's Ryzen AI series laptop chip
- Qualcomm's Snapdragon X Elite laptop chip

---

## üîÑ Return to Von Neumann Architecture

**Vigtig Pointe:**
- P√• trods af al den hypede specialiserede hardware til AI computing, adherer de fleste moderne computere stadig fundamentalt til Von Neumann arkitekturen p√• system niveau
- Uanset om GPUs, TPUs, eller NPUs er integreret i computere, vil denne hardware stadig:
  - Forbinde til CPUs via bus systemet
  - Dele unified memory address space
  - Blive ultimate managed og koordineret af CPUs

**Analog:**
- CPU forbliver "executive chef" der koordinerer systemet
- Specialiserede processorer fungerer som h√∏jt kvalificerede sous chefs der h√•ndterer specifikke opgaver

**Von Neumann Architecture's Geni:**
- Ligger ikke i specifikke komponenter, men i dens **modul√¶re design**
- Forts√¶tter med at accommodate nye typer processing units som computational needs udvikler sig
- **Factorio analogi:** Mens nye assembly lines m√•ske skal bygges til at producere nye typer produkter introduceret af opdateringer til spillet, vil bus systemet forblive den gyldne standard arkitektur hvis du vil have din fabrik til at v√¶re scalable og produktiv

---

## üìã Opsummering til Mini Projekt

### Vigtige Koncepter at Overveje:

1. **Hardware Valg** ‚úÖ
   - Overvej hvilken hardware din AI model kr√¶ver
   - CPU: Til simple modeller eller development
   - GPU: Til st√∏rre modeller der kr√¶ver parallel processing
   - NPU: Hvis du deployer p√• edge devices

2. **Memory Requirements** ‚úÖ
   - Overvej hvor meget memory din model kr√¶ver
   - GPU memory er typisk begr√¶nset sammenlignet med system memory
   - Overvej model st√∏rrelse og batch size

3. **Performance Optimization** ‚≠ê
   - Overvej at bruge GPU hvis tilg√¶ngelig
   - Test performance p√• forskellige hardware typer
   - Dokumenter performance forskelle

4. **Deployment Considerations** ‚úÖ
   - EC2 serveren har muligvis ikke dedikeret GPU
   - CPU-baseret inference kan v√¶re langsommere
   - Overvej model st√∏rrelse og kompleksitet

### Hardware til EC2 Server:

**Hvad vi har:**
- EC2 serveren har sandsynligvis standard CPU (ingen dedikeret GPU)
- Python 3.12.3 og Docker installeret
- Nok memory til mindre modeller

**Anbefalinger:**
- Brug lightweight modeller der kan k√∏re p√• CPU
- Overvej at bruge HuggingFace modeller der er optimeret til CPU
- Test model performance f√∏r deployment
- Dokumenter hardware limitations i rapporten

### Model Selection Tips:

1. **Lightweight Models:**
   - Brug mindre modeller (fx ResNet-18 i stedet for ResNet-152)
   - Overvej quantized modeller (8-bit, 4-bit)
   - Test model st√∏rrelse og inference tid

2. **CPU-Optimized Models:**
   - Nogle modeller er bedre optimeret til CPU
   - HuggingFace har mange CPU-friendly modeller
   - Overvej ONNX runtime for bedre CPU performance

3. **Memory Management:**
   - Monitor memory usage
   - Overvej batch size
   - Implementer proper cleanup

---

## üß™ Exercise: Google Colab Hardware Testing

**Opgave:** K√∏r en AI model p√• forskellige typer hardware leveret af Google Colab.

**Steps:**

1. **Spin up Google Colab:**
   - Interactive playground (essentielt Jupyter Notebook)
   - K√∏r Python kode med forskellige typer hardware (CPU, GPU, TPU)
   - Nok gratis computing timer til at lege med

2. **Test AI Model:**
   - K√∏r image analysis modellen vi brugte i "Wrap AI Models with APIs"
   - Beregn den teoretiske st√∏rrelse af modellen (hint: kan opn√•s ved at beregne antallet af parametre i modellen)

3. **Test Forskellige Hardware:**
   - Skift runtime til forskellige typer hardware (CPU, GPU, TPU)
   - Genk√∏r modellen p√• hver type

4. **Record og Sammenlign:**
   - Optegn tiden modellen har brug for at processe √©t billede
   - Sammenlign tiden p√• tv√¶rs af forskellige typer hardware
   - Dokumenter resultaterne

**Forventede Resultater:**
- GPU vil typisk v√¶re hurtigere end CPU
- TPU kan v√¶re hurtigere end GPU for specifikke workloads
- CPU vil v√¶re langsommest men mest tilg√¶ngelig

---

## ‚úÖ Checklist til Mini Projekt

### Hardware Considerations
- [ ] Hardware valgt (CPU/GPU/NPU)
- [ ] Model st√∏rrelse evalueret
- [ ] Memory requirements verificeret
- [ ] Performance testet p√• target hardware
- [ ] Limitations dokumenteret

### Model Selection
- [ ] Lightweight model valgt (hvis CPU)
- [ ] Model optimeret til target hardware
- [ ] Model st√∏rrelse og parametre dokumenteret
- [ ] Inference tid m√•lt og dokumenteret

### Deployment
- [ ] Model kan k√∏re p√• EC2 serveren
- [ ] Memory usage overv√•get
- [ ] Performance acceptable
- [ ] Hardware limitations forklaret i rapporten

---

## üîó Ressourcer

### Von Neumann Architecture
- Von Neumann Architecture: https://en.wikipedia.org/wiki/Von_Neumann_architecture
- Computer Architecture Basics: https://www.tutorialspoint.com/computer_fundamentals/computer_architecture.htm

### GPU Computing
- NVIDIA CUDA: https://developer.nvidia.com/cuda-toolkit
- GPU vs CPU: https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/

### TPU
- Google TPU: https://cloud.google.com/tpu/docs
- TPU vs GPU: https://cloud.google.com/tpu/docs/tpus

### NPU
- Apple Neural Engine: https://www.apple.com/machine-learning/
- Qualcomm AI Engine: https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-ai-engine

### Model Optimization
- HuggingFace Models: https://huggingface.co/models
- ONNX Runtime: https://onnxruntime.ai/
- Model Quantization: https://huggingface.co/docs/transformers/quantization

### Google Colab
- Google Colab: https://colab.research.google.com/
- Colab GPU/TPU: https://colab.research.google.com/notebooks/gpu.ipynb

---

## üéØ Konklusion

**Vigtige Takeaways:**

1. **Von Neumann Architecture:**
   - St√∏rstedelen af computere bruger stadig denne 80-√•rige arkitektur
   - Modul√¶rt design g√∏r det muligt at tilf√∏je nye processing units

2. **CPU Limitations:**
   - Excellerer ved sequential processing
   - Ikke ideel til massive parallel computation
   - Memory bus bottleneck for AI workloads

3. **Specialized Hardware:**
   - **GPU:** Parallel processing, high-bandwidth memory
   - **TPU:** H√∏jt specialiseret til AI, prim√¶rt i data centers
   - **NPU:** Power-efficient, til edge devices

4. **For Mini Projekt:**
   - Overvej hardware limitations
   - V√¶lg appropriate modeller
   - Test performance
   - Dokumenter hardware valg og begr√¶nsninger

